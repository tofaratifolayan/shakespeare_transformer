{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "``` \n",
    "ROMEO:\n",
    "And from the embracement be spokes to stand,\n",
    "As we shall breathest to the market-fairly maid\n",
    "So month in my father, I may see thee not my side\n",
    "And love the prisoner like a cradist of my daughter.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above text is not a lost work of Shakespeare but a fully generated text by a GPT2-like model I trained on my laptop in less than 20 minutes. Today, in this tutorial, we will follow an implementation of the \"Attention Is All You Need\" paper, so that you can generate your own Shakespeare at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataset import getData, getVocabSize\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "from utils import train, inference\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define all the parameters used for training and to describe the model. Please feel free to modify any parameters described except for certain marked with ``DO NOT MODIFY``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "\n",
    "    # Parameters to modify:\n",
    "    batch_size: int = 64  # How many batches per training step\n",
    "    max_iters: int = 2000  # Total of training iterations\n",
    "    learning_rate: float=1e-3 # Learning rate\n",
    "    grad_clip: float=1.0 # Maximium magnitude of gradient\n",
    "    eval_interval: int=50 # How often to evaluate the model\n",
    "    eval_iters: int=10 # Number of iterations to average for evaluation\n",
    "    seed: int=1337 # Random seed (can change the results)\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # These are responsible for correct training given GPU (DO NOT MODIFY)\n",
    "    dtype: str =  'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "    scaler = torch.amp.GradScaler(device,enabled=(dtype == 'float16'))\n",
    "    \n",
    "    # Populated by the script (DO NOT MODIFY)\n",
    "    train_dataloader: None\n",
    "    test_dataloader: None\n",
    "    optimizer: None\n",
    "\n",
    "class ModelConfig:\n",
    "    context_length: int = 256 # Number of tokens used for predicition\n",
    "    vocab_size: int = -1 # Number of words in the vocab (DO NOT MODIFY; changing the number here can make the model only recognize limited number of words!!!)\n",
    "    n_layer: int = 6 # Depth of the Transformer model (here: 6 Transformer Blocks)\n",
    "    n_head: int = 6 # Number of heads in the Multi-Head Attention\n",
    "    n_embd: int = 384 # Embedding dimension\n",
    "    dropout: float = 0.2 # Fraction used for drop-out; lower fraction -> more robust, but longer training (requires adjustment to the training time)\n",
    "    bias: bool = False # Whether or not to use a bias in the transformers layers\n",
    "    compile: bool = False # Whether to use the torch.compile (slows in the beginning of the training; faster training)\n",
    "    attn_dim: int = n_embd//n_head # Attention dimension (DO NOT MODIFY; changing the number here can break the model)\n",
    "\n",
    "\n",
    "model_config = ModelConfig()\n",
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define CUDA optimizations. This can controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. It offers a significant speed-up, but might not be available on older GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(train_config.seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading function. Here, we get the necessary vocabulary for the training and perform a simple training/testing split. No need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = os.path.join('data', 'Shakespeare')\n",
    "model_config.vocab_size = getVocabSize(data_dir)\n",
    "train_config.train_dataloader, train_config.test_dataloader = getData(data_dir,model_config,train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple definition of a feed-forward layer. No need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feed forward network\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.n_embd * 4, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``attention(self,q,k,v,T)`` of the Attention Module\n",
    "\n",
    "Below, we define the attention layer of the Transformer model. Here, you need to implement the attention mechanism. We define the attention as:\n",
    "$$ Attention(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "Nevertheless, the original attention can easily overfit to the data. To allivate that, we introduce an additional dropout layer. For your convenience, we split the implementation into two steps:\n",
    "$$weights = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "$$attention = \\text{softmax}(\\text{dropout}(weights))V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.Wk = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.Wv = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.context_length, config.context_length, requires_grad=False)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        return self.attention(q,k,v,T)\n",
    "    \n",
    "    def attention(self,q,k,v,T):\n",
    "        # Your code goes here\n",
    "        # (1) Implement the attention operation defined by the equation #1 in the markdown to compute weights.\n",
    "        # (2) Implement the attention operation defined by the equation #2 to compute the full attention\n",
    "        dk = k.size(-1)\n",
    "        weights = (q @ k.transpose(-2, -1)) / math.sqrt(dk) # (1) Equation #1 goes here\n",
    "        weights = weights.masked_fill(self.mask[:T,:T] == 0, float('-inf')) # Do not modify this line; \n",
    "        # In the above line, we limit the attention to the correct window\n",
    "        weights = self.dropout(F.softmax(weights, dim=-1))\n",
    "        attention = weights @ v # (2) Equation #2 goes here\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``forward(self,x)`` of the MultiHeadAttention Module\n",
    "\n",
    "Below, we define the multi-head attention layer of the Transformer model. Here, you need to implement the multi-head attention mechanism defined as:\n",
    "$$MultiHead(x) = \\text{Dropout}(\\text{Concat}(\\text{head}_1, ..., \\text{head}_{\\text{heads}})W^O),$$\n",
    "$$ \\text{where head}_i = \\text{Attention}(x)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-head Attention ï½œ\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([Attention(config) for _ in range(self.config.n_head)])\n",
    "        self.projection_layer = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE GOES HERE\n",
    "        # Compute attention for each head\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate along the embedding dimension\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        # Project the concatenated heads and apply dropout\n",
    "        output = self.dropout(self.projection_layer(concatenated))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are able to define the standard Transformer Block. No changes required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer Block ï½œ\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForwardNetwork(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``__init__`` of Positional Encoding\n",
    "\n",
    "Below, we define the Positional Encoding of the Transformer architecture. The positional encoding gives a specific value based on the token position in the input data. Therefore, a positional encoding can be seen as a feature defined only based on the position of each token. We can precompute it as:\n",
    "$$PE(pos,2i) = \\sin(\\text{pos}/div)$$\n",
    "$$PE(pos,2i+1) = \\cos(\\text{pos}/div),$$\n",
    "where $div=10000^{2i/dmodel}$ and the first equation defined the positional encoding for even tokens and the second one defines the encoding for the odd tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(0, config.context_length, requires_grad=False).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, config.n_embd, 2) * (math.log(10000.0) / config.n_embd))\n",
    "        pe = torch.zeros(config.context_length, config.n_embd, requires_grad=False)\n",
    "        # YOUR CODE GOES HERE TO POPULATE PE\n",
    "        pe[:, 0::2] = torch.sin(pos / div)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(pos / div)  # Apply cosine to odd indices\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pe[:x.size(1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our model. We combine all our blocks into final Transfomer Model consisting of multiple Transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ï½œ\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_embedding = PositionalEncoding(config)\n",
    "        self.transformer_blocks = nn.Sequential(*(\n",
    "                [TransformerBlock(config) for _ in range(config.n_layer)] +\n",
    "                [nn.LayerNorm(config.n_embd)]\n",
    "        ))\n",
    "        self.model_out_linear_layer = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.context_length = config.context_length\n",
    "\n",
    "    def forward(self, idx:torch.Tensor):\n",
    "        _, T = idx.shape\n",
    "        pos_emb = self.pos_embedding(idx)\n",
    "        tok_emb = self.tok_embedding(idx)\n",
    "\n",
    "        x = self.transformer_blocks(self.drop(tok_emb+pos_emb))\n",
    "        logits = self.model_out_linear_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can initialize the model and, optionally, compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Model(model_config).to(train_config.device)\n",
    "if model_config.compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start the optimization process and start our training! This will take a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7a5774283e49f4a1f738a22c2ca8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the optimizer and train; Losses updated every eval_interval steps\n",
    "train_config.optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.learning_rate)\n",
    "train(model,train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can save the model for further use. We will use this to show you how to load a model in other applications below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model/model.ckpt')\n",
    "with open('model/model_config.pkl','wb') as f:\n",
    "    pickle.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration used for inference. Feel free to modify it to your liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig():\n",
    "    seed:int=0 # Random seed (impacts the output)\n",
    "    start:str=\"ROMEO:\" # Starting prompt to generate from\n",
    "    temperature:float = 0.7 # Degree of 'creativity': 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "    max_new_tokens:int=250 # Length of the generated sequence in tokens\n",
    "    top_k:int=None  # Retain only the top k most likely tokens, clamp others to have 0 probability (None - no clamp)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we define our CUDA operations if possible. Use the same CUDA config as the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "torch.manual_seed(inference_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the model and optionally compile it. As the `meta_path`, we load the information about the vocabulary we trained the model on to help it with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and hyperparameters ï½œ\n",
    "with open('model/model_config.pkl', 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "\n",
    "model = Model(model_config)\n",
    "if model_config.compile:\n",
    "    model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('model/model.ckpt', weights_only=True),strict=False)\n",
    "model.eval()\n",
    "model.to(inference_config.device)\n",
    "\n",
    "inference_config.meta_path = os.path.join('data', 'Shakespeare', 'meta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can generate your text here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "print(inference(model, inference_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how big the model is, you can run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, print model total of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engs106_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
